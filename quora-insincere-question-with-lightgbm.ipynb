{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train.csv').head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "\n",
       "   target  \n",
       "0       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>[How, did, Quebec, nationalists, see, their, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "\n",
       "   target                                             tokens  \n",
       "0       0  [How, did, Quebec, nationalists, see, their, p...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('popular')\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# text cleaning & tokenization\n",
    "def tokenize(text, stop_set = None, lemmatizer = None):\n",
    "    \n",
    "    # clean text\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    #text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*', ' ', text) # remove hyperlink,subs charact in the brackets\n",
    "    text = re.sub(\"[\\r\\n]\", ' ', text) # remove new line characters\n",
    "    #text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    # use TweetTokenizer instead of word_tokenize -> to prevent splitting at apostrophies\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(text)\n",
    "    \n",
    "    # retain tokens with at least two words\n",
    "    tokens = [token for token in tokens if re.match(r'.*[a-z]{1,}.*', token)]\n",
    "    \n",
    "    # remove stopwords - optional\n",
    "    # removing stopwords lost important information\n",
    "    if stop_set != None:\n",
    "        tokens = [token for token in tokens if token not in stop_set]\n",
    "    \n",
    "    # lemmmatization - optional\n",
    "    if lemmatizer != None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "train['tokens'] = train['question_text'].map(lambda x: tokenize(x))\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-05792c9b1ebd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnews_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "news_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google News Embeddings\n",
    "# replace not found words\n",
    "to_remove = ['to','of','and', 'a']\n",
    "\n",
    "#replace_dict = {}\n",
    "#replace_dict = {'quora':'Quora', 'i\\'ve':'I\\'ve', 'instagram':'Instagram', 'upsc':'UPSC', 'bitcoin':'Bitcoin', 'trump\\'s':'Trump',\n",
    "#               'mbbs':'MBBS', 'whatsapp':'WhatsApp', 'favourite':'favorite', 'ece':'ECE', 'aiims':'AIIMS', 'colour':'color',\n",
    "#               'doesnt':'doesn\\'t','centre':'center','sbi':'SBI','cgl':'CGL','iim':'IIM','btech':'BTech'}\n",
    "\n",
    "replace_dict = {'favourite':'favorite', 'bitcoin':'Bitcoin', 'colour':'color', 'doesnt':'doesn\\'t', 'centre':'center', 'Quorans':'Quora',\n",
    "               'travelling':'traveling', 'counselling':'counseling', 'didnt':'didn\\'t', 'btech':'BTech','isnt':'isn\\'t',\n",
    "               'Shouldn\\'t':'shouldn\\'t', 'programme':'program', 'realise':'realize', 'Wouldn\\'t':'wouldn\\'t', 'defence':'defense',\n",
    "               'Aren\\'t':'aren\\'t', 'organisation':'organization', 'How\\'s':'how\\'s', 'e-commerce':'ecommerce', 'grey':'gray',\n",
    "               'bitcoins':'Bitcoin', 'honours':'honors', 'learnt':'learned', 'licence':'license', 'mtech':'MTech', 'colours':'colors',\n",
    "               'e-mail':'email', 't-shirt':'tshirt', 'Whatis':'What\\'s', 'theatre':'theater', 'labour':'labor', 'Isnt':'Isn\\'t',\n",
    "               'behaviour':'behavior','aadhar':'Aadhar', 'Qoura':'Quora', 'aluminium':'aluminum'}\n",
    "\n",
    "def clean_token(tokens, remove_list, re_dict, embedding):\n",
    "    \n",
    "    c_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in remove_list:\n",
    "            token2 = token\n",
    "            if token2 in embedding:\n",
    "                c_tokens.append(token2)\n",
    "            elif token2 in re_dict:\n",
    "                token2 = re_dict[token2]\n",
    "                c_tokens.append(token2)\n",
    "            else:    \n",
    "                # apostrophe\n",
    "                if token2.endswith('\\'s'):\n",
    "                    token2 = token2[:-2]\n",
    "                    \n",
    "                if (token2.endswith('s')) & (token2[:-1] in embedding):\n",
    "                    token2 = token2[:-1]\n",
    "                    \n",
    "                # break dash\n",
    "                if \"-\" in token2:\n",
    "                    token2 = token2.split('-')\n",
    "                    c_tokens += token2\n",
    "                else:\n",
    "                    c_tokens.append(token2)\n",
    "        \n",
    "\n",
    "    return c_tokens\n",
    "\n",
    "train['clean_tokens'] = train['tokens'].map(lambda x: clean_token(x, to_remove, replace_dict, embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_mean(tokens, embedding):\n",
    "    \n",
    "    e_values = []\n",
    "    e_values = [embedding[token] for token in tokens if token in embedding]\n",
    "    \n",
    "    if len(e_values) > 0:\n",
    "        return np.mean(np.array(e_values), axis=0)\n",
    "        #return np.sum(np.array(e_values), axis=0)\n",
    "    else:\n",
    "        #return np.ones(300)*-999\n",
    "        return np.zeros(300)\n",
    "      \n",
    "X = np.vstack(train['clean_tokens'].apply(lambda x: doc_mean(x, embeddings_index)))\n",
    "#X = np.vstack(train['tokens'].apply(lambda x: doc_mean(x, embeddings_index)))\n",
    "\n",
    "y = train['target'].values\n",
    "indices = train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free up RAM\n",
    "import gc\n",
    "\n",
    "del embeddings_index\n",
    "#del train\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, tree, ensemble, metrics, model_selection, exceptions\n",
    "\n",
    "def print_score(y_true, y_pred):\n",
    "    print(' accuracy : ', metrics.accuracy_score(y_true, y_pred))\n",
    "    print('precision : ', metrics.precision_score(y_true, y_pred))\n",
    "    print('   recall : ', metrics.recall_score(y_true, y_pred))\n",
    "    print('       F1 : ', metrics.f1_score(y_true, y_pred))\n",
    "\n",
    "    \n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = model_selection.train_test_split(X, y, indices, test_size = 0.2, random_state = 2019)\n",
    "\n",
    "# train-test split - small sample\n",
    "#X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2, train_size = 0.2, random_state = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free up RAM\n",
    "import gc\n",
    "\n",
    "#del X\n",
    "del y\n",
    "#del train\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate 1 - Achieve high accuracy + Threshold search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "# lgb_c = lgb.LGBMClassifier(learning_rate = 0.02,n_estimators = 2000)\n",
    "\n",
    "# lgb_c.fit(X_train, y_train,\n",
    "#           eval_set = [(X_test, y_test)],\n",
    "#           early_stopping_rounds = 5,\n",
    "#           eval_metric = 'auc',\n",
    "#           verbose = 100)\n",
    "\n",
    "\n",
    "# y_pred = lgb_c.predict(X_test, num_iteration=lgb_c.best_iteration_)\n",
    "# print_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treshold search\n",
    "\n",
    "# y_pred_proba = lgb_c.predict_proba(X_test,num_iteration=lgb_c.best_iteration_)[:,1]\n",
    "\n",
    "# thresholds = []\n",
    "# for thresh in np.arange(0.1, 0.901, 0.01):\n",
    "#     res = metrics.f1_score(y_test, (y_pred_proba > thresh).astype(int))\n",
    "#     thresholds.append([thresh, res])\n",
    "#     # print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "\n",
    "# thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "# best_thresh = thresholds[0][0]\n",
    "# print(\"Best threshold: \", best_thresh)\n",
    "# print(\"Best F1: \", thresholds[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate 2 - Tune scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "# gridParams = {\n",
    "#     'scale_pos_weight': [3, 3.5, 4, 4.5]\n",
    "#     }\n",
    "\n",
    "# lgb_c = lgb.LGBMClassifier(learning_rate = 0.02,n_estimators = 2000)\n",
    "# grid_lgb = model_selection.GridSearchCV(lgb_c, gridParams, scoring='f1', cv = 3)\n",
    "\n",
    "\n",
    "# grid_lgb.fit(X_train, y_train, \n",
    "#           eval_set = [(X_test, y_test)],\n",
    "#           early_stopping_rounds = 5,\n",
    "#           eval_metric = 'auc',\n",
    "#           verbose = 500)\n",
    "\n",
    "# print(grid_lgb.best_params_)\n",
    "# print(grid_lgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's auc: 0.933508\tvalid_0's binary_logloss: 0.137978\n",
      "[1000]\tvalid_0's auc: 0.941606\tvalid_0's binary_logloss: 0.12916\n",
      "[1500]\tvalid_0's auc: 0.945184\tvalid_0's binary_logloss: 0.125095\n",
      "[2000]\tvalid_0's auc: 0.947547\tvalid_0's binary_logloss: 0.122116\n",
      "[2500]\tvalid_0's auc: 0.949421\tvalid_0's binary_logloss: 0.119909\n",
      "[3000]\tvalid_0's auc: 0.950506\tvalid_0's binary_logloss: 0.118697\n",
      " accuracy :  0.9529179825820653\n",
      "precision :  0.7074844074844074\n",
      "   recall :  0.41777668651402616\n",
      "       F1 :  0.525336729574312\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_c = lgb.LGBMClassifier(learning_rate = 0.04,n_estimators = 3200, boosting_type = 'dart')\n",
    "\n",
    "lgb_c.fit(X_train, y_train,\n",
    "          eval_set = [(X_test, y_test)],\n",
    "          eval_metric = 'auc',\n",
    "          verbose = 500)\n",
    "\n",
    "\n",
    "y_pred = lgb_c.predict(X_test, num_iteration=lgb_c.best_iteration_)\n",
    "print_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold:  0.25\n",
      "Best F1:  0.6142369873270291\n"
     ]
    }
   ],
   "source": [
    "# treshold search\n",
    "\n",
    "y_pred_proba = lgb_c.predict_proba(X_test,num_iteration=lgb_c.best_iteration_)[:,1]\n",
    "\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.91, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(y_test, (y_pred_proba > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    # print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "\n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)\n",
    "print(\"Best F1: \", thresholds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's auc: 0.935455\tvalid_0's binary_logloss: 0.172922\n",
      "[1000]\tvalid_0's auc: 0.942339\tvalid_0's binary_logloss: 0.160802\n",
      "[1500]\tvalid_0's auc: 0.945623\tvalid_0's binary_logloss: 0.154813\n",
      "[2000]\tvalid_0's auc: 0.948037\tvalid_0's binary_logloss: 0.150315\n",
      "[2500]\tvalid_0's auc: 0.949826\tvalid_0's binary_logloss: 0.146577\n",
      "[3000]\tvalid_0's auc: 0.950793\tvalid_0's binary_logloss: 0.144228\n",
      " accuracy :  0.9459661211599196\n",
      "precision :  0.5540647982508448\n",
      "   recall :  0.6844269842244184\n",
      "       F1 :  0.6123850061787726\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_c_weight = lgb.LGBMClassifier(learning_rate = 0.04,n_estimators = 3200, boosting_type = 'dart', scale_pos_weight = 3.5)\n",
    "\n",
    "lgb_c_weight.fit(X_train, y_train,\n",
    "          eval_set = [(X_test, y_test)],\n",
    "          eval_metric = 'auc',\n",
    "          verbose = 500)\n",
    "\n",
    "\n",
    "y_pred_weight = lgb_c_weight.predict(X_test, num_iteration=lgb_c_weight.best_iteration_)\n",
    "print_score(y_test, y_pred_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_labels = lgb_c_weight.predict(X, num_iteration=lgb_c_weight.best_iteration_)\n",
    "p_proba = lgb_c_weight.predict_proba(X, num_iteration=lgb_c_weight.best_iteration_)\n",
    "\n",
    "output_np = np.concatenate((p_labels.reshape(len(p_labels), 1), p_proba), axis = 1)\n",
    "output = pd.DataFrame(output_np)\n",
    "output.to_csv('label with proba.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
